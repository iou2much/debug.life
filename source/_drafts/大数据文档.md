## 项目代码结构说明![图](http://7xk2xn.com1.z0.glb.clouddn.com/structure.png)## 项目组件逻辑架构图![图](http://7xk2xn.com1.z0.glb.clouddn.com/architecture.png)## 采集模块### 关键文档- doc/第三方对接文档/数据采集模块接入说明.docx - doc/工程管理/02设计/02数据库设计/数据分析系统-数据结构总表.xlsx### 关键代码#### 客户端：> ##### Js : - pvcollector.js ：加载入口- pvcollector1.js  ：具体埋点采集代码> 以上js的接入方式，以及所采集到的数据字段请参照上述关键文档。> ##### Android: (原开发者 古国丰/吕荣砖)> ##### iOS:(原开发者 张鹏宇/吕荣砖)> 移动端的接入也请参照文档，代码说明文档请让移动端的同事支持。#### 服务端pv-webserver - 启动脚本：bin/run.sh>此脚本会启动两个java进程，- MobileWebServer(原开发者 赵科/李波/吕荣砖):  监听9999端口，接收js Ajax接口请求。- HttpStaticFileServer(原开发者 李波/吕荣砖)：监听8080端口，接收Android/iOS上传的文件。## 初始化脚本- initial  - hbase-initialize  - hive-initialize  - oracle-initialize   > 需要先执行hbase的表创建再执行hive的。## 数据导入工程 LogImporter (原开发者 李波/吕荣砖)### 调用脚本在bin下，平常使用得较多的是：generalImport.sh  csvImport.sh，它们分别调用的是GeneralFileImporter 、 CSVDataImporter 这两个主类。### 关键配置conf/DEV下：- hbase-site.xml ：hbase连接配置- log4j.properties：日志配置- spring-hbase.xml：文件与hbase 的字段结构对应关系配置#### 示例-json格式数据配置说明 ```xml	<!-- pv采集 -->    <!-- bean的id需要在调用的时候指定 -->    <!-- 导入csv时需要指定MetaDataOfData这个class-->	<bean id="SWS_RAW_PV" class="com.berchina.data.util.MetaDataOfData">		<!-- 连接到hbase-->		<property name="usingHBase" value="true" />		<!-- 不生效-->		<property name="dataType" value="json" />		<!-- 目标命名空间-->		<property name="targetNamespace" value="SWS" />		<!-- 目标表名-->		<property name="targetTable" value="raw_pv" />		<!-- 主键组合 -->		<property name="primaryKey" value="log_time,app_id" />		<!-- 非空字段 -->		<property name="requriedKey" value="log_time" />		<!-- 日志表（记录已导入过的文件名） -->		<property name="importedLogTable" value="log_imported_file" />		<!-- 日志路径（记录已导入过的文件名）-->		<property name="importedLogPath" value="/home/bigdata/logs/importer/imported" />		<!-- 数据存放路径 -->		<property name="dataPath" value="/home/bigdata/data/pv/SWS" />		<!-- 导入文件后缀 -->		<property name="fileSuffix" value="log" />		<!-- 对应字段名 -->		<property name="fields"				  value="basic:app_id,basic:log_time, basic:ip ,basic:user_id , basic:net_type , basic:net_carrier , basic:url ,basic:ref_url , basic:landscape_mode ,basic:module, agent:resolution , agent:ua_brand , agent:ua_model ,agent:browser,agent:ua_os , agent:ua_os_ver , agent:ua_market , agent:app_ver ,action:view_act , action:act_type , action:act_para , action:act_res,extend:source_id,extend:shop_id,extend:form_shop_id,extend:is_mobile,extend:session_id,extend:product_id,extend:keywords,extend:search_product_count,page_title,extend:os_id,extend:browser_id,extend:sys_is_new_user" />	</bean>```#### 示例-csv格式数据配置说明与上边json格式有共性的选项不再提及```xml        <!-- 用户信息表 -->        <!-- bean的id需要在调用的时候指定 -->        <!-- 导入csv时需要指定MetaDataOfPlainData这个class-->        <bean id="O_SWS_MEMBER" class="com.berchina.data.util.MetaDataOfPlainData">                <property name="usingHBase" value="true" />                <property name="dataType" value="json" />                <property name="targetNamespace" value="SWS" />                <property name="targetTable" value="o_sws_member" />                <property name="primaryKey" value="memberno" />                <property name="requriedKey" value="memberno" />                <property name="importedLogTable" value="log_imported_file" />                <property name="importedLogPath" value="/home/bigdata/logs/importer/imported" />                <property name="dataPath"                                  value="/home/bigdata/data/import/O_SWS_MEMBER" />                <property name="fileSuffix" value="dat" />                <!-- hbase中的对应字段 -->                <property name="fields"                                  value="info:memberno ,info:loginname ,info:loginpwd ,info:membername ,info:idtype ,info:idno ,info:gender ,info:phoneno ,info:email ,info:address ,info:memberlevel ,info:memberstatus ,info:createtime ,info:lastlogintime ,info:logintimes ,info:errorlogintimes ,info:customersid ,info:fromtype ,info:exstate ,info:iscollectsingle ,info:custno ,info:iscustserver ,info:membertype ,info:issaveauth ,info:ismbauth ,info:roleid ,info:paypwd ,info:secquestion1 ,info:secanswer1 ,info:secquestion2 ,info:secanswer2 ,info:secquestion3 ,info:secanswer3 ,info:lastlogindate ,info:accountauth ,info:errorpaytimes ,info:imgurl ,info:islockpaypassword ,info:subtype ,info:lockpaypassworddate ,info:lockloginpassworddate ,info:channel ,info:isopenemall ,info:isopenepay ,info:institutionid ,info:isbatch ,info:data_time" />                <!-- csv数据中的列分隔符 -->                <property name="delimiter" value="\|@\|" />                <!-- csv中的对应字段,位置必须严格对应 -->                <property name="fieldsPosition"                                  value="memberno ,loginname ,loginpwd ,membername ,idtype ,idno ,gender ,phoneno ,email ,address ,memberlevel ,memberstatus ,createtime ,lastlogintime ,logintimes ,errorlogintimes ,customersid ,fromtype ,exstate ,iscollectsingle ,custno ,iscustserver ,membertype ,issaveauth ,ismbauth ,roleid ,paypwd ,secquestion1 ,secanswer1 ,secquestion2 ,secanswer2 ,secquestion3 ,secanswer3 ,lastlogindate ,accountauth ,errorpaytimes ,imgurl ,islockpaypassword ,subtype ,lockpaypassworddate ,lockloginpassworddate ,channel ,isopenemall ,isopenepay ,institutionid ,isbatch ,data_time" />        </bean>```#### 核心代码说明在src/main/java/com/berchina/下- data  - extrator    - PatternExtrator.java (csv格式数据结构解析类)    - DataExtractor.java (json格式数据结构解析类)  - hbase    - HBaseUtil.java (hbase接口调用封装)    - PooledHTableFactory.java (hbase连接池)  - pojo (导入mongodb格式实体类)  - trigger    - FileImporter.java (文件导入抽象类)    - GeneralFileImporter.java(json格式导入实现类)    - CSVDataImporter.java(csv格式导入实现类)  - util    - DataSinker.java(hbase插入util)    - DateUtil.java(日期util)    ## 数据预处理 Analyzer工程（原作者 李波）### 启动脚本> bin/run-preprocess.sh### 关键代码- resources- src/com/berchina/data  - IP (ip解析包)    - IPEntry.java    - IPFile.java    - IPLocation.java    - IPSeeker.java : (入口)    - LogFactory.java    - Message.java    - Util.java  - mr (预处理)    - DmActiveness.java (弃置)    - PreprocessDriver.java (处理日期/ip地域)  - utils     - BerPut.java    - DateUtil.java## Hive分析 -- HiveQL(原作者 兰州大数据项目成员)此目录全是具体的分析SQL语句.## 任务调度及数据导出 dataExporter(原作者 兰州大数据项目成员)### 关键代码说明- bin  - alert_msg.sh (任务状态失败报警)  - send_msg.sh (短信发送)  - sync_pv_files.sh (同步pv数据文件)  - job_logger.sh (任务状态设置)  - job_status_setter.sh (任务状态设置)  - importFile.sh (调用LogImporter工程导入数据)  - runHQL.sh (运行Hive SQL)  - check_files.sh (检测数据文件是否已经推送)  - check_job.sh (检查任务状态)  - check_self.sh (检查自身任务状态)- utils  - import2HBase.kjb (导入数据子任务)  - importPVFile2HBase.kjb (导入pv数据子任务)  - runHql.kjb (运行Hive SQL子任务)  - runhql_sync.kjb (运行Hive SQL并同步数据至Oracle子任务)  - set_date.ktr (设置当前日期)  - sync.kjb (同步数据至Oracle子任务)## IEC(原作者 陈文勇/杨志红/兰州大数据项目成员)报表工程